{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly4xjbn6W_fN"
      },
      "source": [
        "Import all required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpJulPBiXC25"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "import base64\n",
        "import struct # For converting bytes to float\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiCIVzeDWhNk"
      },
      "source": [
        "This portion is for the Direct PyTorch/TensorFlow Integration.\n",
        "This should be finished around chunk 43."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBdbONxRJ38"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "import base64\n",
        "import struct\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    API_KEY = userdata.get('OCCYBYTE_API_KEY')\n",
        "    if not API_KEY:\n",
        "        print(\"Warning: OCCYBYTE_API_KEY secret found but is empty. Falling back to env var or placeholder.\")\n",
        "        API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "except ImportError:\n",
        "    print(\"Warning: google.colab not found. Using OCCYBYTE_API_KEY environment variable or placeholder.\")\n",
        "    API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "BASE_URL = \"https://entropy.occybyte.com/api/eris/invoke\"\n",
        "# *** Reduced Chunk Size ***\n",
        "MAX_BYTES_PER_REQUEST = 16 * 1024 # 16384 bytes\n",
        "FLOAT_PRECISION = torch.float32\n",
        "BYTES_PER_FLOAT = torch.finfo(FLOAT_PRECISION).bits // 8\n",
        "# Keep timeout moderate now that we have retries\n",
        "REQUEST_TIMEOUT = 15\n",
        "CHUNK_DELAY_SECONDS = 0.1\n",
        "# *** Added Retry Logic ***\n",
        "MAX_RETRIES = 3 # Number of retries per chunk\n",
        "RETRY_DELAY_SECONDS = 2 # Wait time between retries\n",
        "\n",
        "# --- Robust Quantum Byte Fetching with Chunking and Retries ---\n",
        "\n",
        "def fetch_quantum_bytes(total_bytes_needed: int, api_key: str) -> bytes | None:\n",
        "    \"\"\"\n",
        "    Fetches raw quantum bytes from the ERIS API, handling chunking and retries.\n",
        "    \"\"\"\n",
        "    if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"Error: API Key not configured.\")\n",
        "        return None\n",
        "\n",
        "    if total_bytes_needed <= 0:\n",
        "        return b''\n",
        "\n",
        "    all_fetched_bytes = bytearray()\n",
        "    bytes_remaining = total_bytes_needed # Track bytes still needed conceptually\n",
        "    # Calculate expected bytes based on actual need, not API chunk size\n",
        "    target_fetched_length = 0\n",
        "\n",
        "    # Calculate number of chunks based on the NEW max size\n",
        "    num_chunks = math.ceil(total_bytes_needed / MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "    print(f\"Starting fetch for {total_bytes_needed} bytes in {num_chunks} chunk(s) (max {MAX_BYTES_PER_REQUEST} bytes/chunk, {MAX_RETRIES} retries/chunk)...\")\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        # Calculate how many bytes we *intend* to get in this chunk based on remaining need\n",
        "        bytes_to_request_this_chunk = min(total_bytes_needed - len(all_fetched_bytes), MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "        if bytes_to_request_this_chunk <= 0:\n",
        "             print(\"Warning: Calculation resulted in 0 bytes requested for a chunk. This might indicate an issue.\")\n",
        "             continue # Skip if somehow we think we need 0 bytes for this chunk\n",
        "\n",
        "        url = f\"{BASE_URL}?size={bytes_to_request_this_chunk}\"\n",
        "        headers = {\"X-API-Key\": api_key}\n",
        "\n",
        "        # --- Retry Loop ---\n",
        "        success = False\n",
        "        for attempt in range(MAX_RETRIES + 1): # +1 for the initial try\n",
        "            try:\n",
        "                print(f\"  Requesting chunk {i+1}/{num_chunks} ({bytes_to_request_this_chunk} bytes), attempt {attempt+1}/{MAX_RETRIES+1}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
        "                response.raise_for_status() # Check for HTTP errors (4xx, 5xx)\n",
        "\n",
        "                json_response = response.json()\n",
        "                if \"data\" in json_response:\n",
        "                    base64_data = json_response[\"data\"]\n",
        "                    chunk_bytes = base64.b64decode(base64_data)\n",
        "\n",
        "                    # We need *at least* bytes_to_request_this_chunk, but API might give more\n",
        "                    if len(chunk_bytes) < bytes_to_request_this_chunk:\n",
        "                         # Treat this as a failure for retry purposes\n",
        "                         raise ValueError(f\"API returned fewer bytes ({len(chunk_bytes)}) than requested ({bytes_to_request_this_chunk})\")\n",
        "\n",
        "                    all_fetched_bytes.extend(chunk_bytes)\n",
        "                    print(f\"  Received {len(chunk_bytes)} bytes for chunk {i+1}. Total fetched so far: {len(all_fetched_bytes)}\")\n",
        "                    success = True # Mark success for this chunk\n",
        "                    break # Exit retry loop on success\n",
        "\n",
        "                else:\n",
        "                    # Treat missing 'data' field as a failure for retry\n",
        "                    raise ValueError(\"'data' field not found in API response\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                 print(f\"  Attempt {attempt+1} timed out after {REQUEST_TIMEOUT} seconds.\")\n",
        "                 # Continue to next retry attempt if not the last one\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"  Attempt {attempt+1} failed with network/HTTP error: {e}\")\n",
        "                # Can check e.response.status_code here for specific handling if needed\n",
        "                # Continue to next retry attempt if not the last one\n",
        "            except (ValueError, TypeError, base64.binascii.Error) as e:\n",
        "                 print(f\"  Attempt {attempt+1} failed during data processing: {e}\")\n",
        "                 # These are less likely to be transient, but retry anyway\n",
        "            except Exception as e: # Catch any other unexpected errors\n",
        "                 print(f\"  Attempt {attempt+1} failed with unexpected error: {e}\")\n",
        "\n",
        "            # If not the last attempt and not successful, wait before retrying\n",
        "            if not success and attempt < MAX_RETRIES:\n",
        "                print(f\"  Waiting {RETRY_DELAY_SECONDS}s before retrying...\")\n",
        "                time.sleep(RETRY_DELAY_SECONDS)\n",
        "            elif not success and attempt == MAX_RETRIES:\n",
        "                 print(f\"Chunk {i+1} failed after {MAX_RETRIES+1} attempts. Aborting fetch.\")\n",
        "                 return None # Failed to get this chunk after all retries\n",
        "\n",
        "        # --- End Retry Loop ---\n",
        "\n",
        "        # Add a small delay before the next chunk request if successful\n",
        "        if success and i < num_chunks - 1:\n",
        "             time.sleep(CHUNK_DELAY_SECONDS)\n",
        "\n",
        "    # Final check: Ensure we have accumulated AT LEAST the total bytes needed\n",
        "    # This check is crucial because the API might return more bytes per chunk\n",
        "    if len(all_fetched_bytes) < total_bytes_needed:\n",
        "        print(f\"Error: Fetching completed, but total bytes received ({len(all_fetched_bytes)}) is less than required ({total_bytes_needed}).\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Successfully fetched {len(all_fetched_bytes)} total bytes (needed {total_bytes_needed}).\")\n",
        "    return bytes(all_fetched_bytes)\n",
        "\n",
        "\n",
        "# --- Quantum Weight Initialization Function (Unchanged from previous version) ---\n",
        "\n",
        "def quantum_uniform_init_(tensor: torch.Tensor, api_key: str, a: float = 0.0, b: float = 1.0):\n",
        "    \"\"\"\n",
        "    Initializes the input tensor with quantum random numbers following a uniform\n",
        "    distribution U(a, b). Modifies the tensor in-place. Uses chunking/retries.\n",
        "    \"\"\"\n",
        "    if not isinstance(tensor, torch.Tensor):\n",
        "        raise TypeError(\"Input must be a PyTorch Tensor\")\n",
        "    if a >= b:\n",
        "         raise ValueError(\"Lower bound 'a' must be less than upper bound 'b'\")\n",
        "\n",
        "    num_elements = tensor.numel()\n",
        "    if num_elements == 0:\n",
        "         print(\"Tensor has no elements. Skipping initialization.\")\n",
        "         return\n",
        "\n",
        "    bytes_needed = num_elements * BYTES_PER_FLOAT\n",
        "\n",
        "    print(f\"\\nInitializing tensor of shape {tensor.shape} ({num_elements} elements, {bytes_needed} bytes needed).\")\n",
        "    all_quantum_bytes = fetch_quantum_bytes(bytes_needed, api_key) # Calls the updated fetcher\n",
        "\n",
        "    if all_quantum_bytes is None:\n",
        "        print(\"Failed to fetch sufficient quantum bytes for initialization. Tensor not modified.\")\n",
        "        return\n",
        "\n",
        "    exact_quantum_bytes = all_quantum_bytes[:bytes_needed]\n",
        "\n",
        "    if len(exact_quantum_bytes) != bytes_needed:\n",
        "         print(f\"Error: After slicing, byte count ({len(exact_quantum_bytes)}) does not match required ({bytes_needed}).\")\n",
        "         return\n",
        "\n",
        "    if BYTES_PER_FLOAT != 4:\n",
        "         raise NotImplementedError(\"Only float32 initialization (4 bytes) is currently implemented with struct.\")\n",
        "\n",
        "    try:\n",
        "        uint_iterator = struct.iter_unpack('<I', exact_quantum_bytes)\n",
        "        max_uint32 = (1 << 32) - 1\n",
        "        quantum_floats_0_1 = torch.tensor(\n",
        "            [float(val) / (max_uint32 + 1) for val, in uint_iterator],\n",
        "            dtype=FLOAT_PRECISION\n",
        "        )\n",
        "\n",
        "        if quantum_floats_0_1.numel() != num_elements:\n",
        "             print(f\"Error: Number of generated floats ({quantum_floats_0_1.numel()}) does not match tensor elements ({num_elements}) after unpacking.\")\n",
        "             return\n",
        "\n",
        "        quantum_uniform_values = a + (b - a) * quantum_floats_0_1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tensor.copy_(quantum_uniform_values.reshape(tensor.shape))\n",
        "\n",
        "        print(f\"Tensor successfully initialized with quantum uniform U({a:.4f}, {b:.4f}).\")\n",
        "\n",
        "    except struct.error as e:\n",
        "        print(f\"Error unpacking bytes into floats: {e}. Tensor not modified.\")\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred during float conversion or assignment: {e}. Tensor not modified.\")\n",
        "\n",
        "\n",
        "# --- Example Usage (Unchanged) ---\n",
        "\n",
        "# Define the layer\n",
        "layer_in_features = 512\n",
        "layer_out_features = 512\n",
        "linear_layer = torch.nn.Linear(layer_in_features, layer_out_features, bias=True)\n",
        "\n",
        "# Calculate default PyTorch uniform bounds\n",
        "k = 1.0 / layer_in_features\n",
        "bound = math.sqrt(k)\n",
        "weight_a, weight_b = -bound, bound\n",
        "bias_a, bias_b = -bound, bound # Default bias bounds are same\n",
        "\n",
        "# Initialize weights\n",
        "print(\"\\n--- Initializing Layer Weights ---\")\n",
        "quantum_uniform_init_(linear_layer.weight.data, API_KEY, a=weight_a, b=weight_b)\n",
        "\n",
        "# Initialize bias\n",
        "if linear_layer.bias is not None:\n",
        "    print(\"\\n--- Initializing Layer Bias ---\")\n",
        "    quantum_uniform_init_(linear_layer.bias.data, API_KEY, a=bias_a, b=bias_b)\n",
        "\n",
        "# Verify (optional)\n",
        "if linear_layer.weight.numel() > 0:\n",
        "     default_weight = torch.nn.Linear(layer_in_features, layer_out_features, bias=False).weight.data\n",
        "     if torch.equal(linear_layer.weight.data, default_weight):\n",
        "          print(\"\\nWarning: Weights seem unchanged from default initialization (Quantum fetch likely failed).\")\n",
        "     else:\n",
        "          print(f\"\\nWeight min/max after quantum init: {linear_layer.weight.min().item():.4f} / {linear_layer.weight.max().item():.4f} (Target range: [{weight_a:.4f}, {weight_b:.4f}])\")\n",
        "\n",
        "if linear_layer.bias is not None and linear_layer.bias.numel() > 0:\n",
        "     default_bias = torch.nn.Linear(layer_in_features, layer_out_features, bias=True).bias.data\n",
        "     if torch.equal(linear_layer.bias.data, default_bias):\n",
        "           print(\"Warning: Bias seems unchanged from default initialization (Quantum fetch likely failed).\")\n",
        "     else:\n",
        "          print(f\"Bias min/max after quantum init: {linear_layer.bias.min().item():.4f} / {linear_layer.bias.max().item():.4f} (Target range: [{bias_a:.4f}, {bias_b:.4f}])\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9TO03BWq_i"
      },
      "source": [
        "This next part is for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyZngQZJRWPD"
      },
      "outputs": [],
      "source": [
        "# --- LLM Text Generation Sampling Example ---\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import transformers\n",
        "import os\n",
        "import base64\n",
        "import struct # For converting bytes to float\n",
        "import time # For potential delays\n",
        "import math # For ceiling calculation\n",
        "\n",
        "# --- Configuration (Needs to be defined before helpers) ---\n",
        "try:\n",
        "    # Use Colab secrets if available\n",
        "    from google.colab import userdata\n",
        "    API_KEY = userdata.get('OCCYBYTE_API_KEY')\n",
        "    if not API_KEY:\n",
        "        print(\"Warning: OCCYBYTE_API_KEY secret found but is empty. Falling back to env var or placeholder.\")\n",
        "        API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "except ImportError:\n",
        "    # Fallback for environments without google.colab\n",
        "    print(\"Warning: google.colab not found. Using OCCYBYTE_API_KEY environment variable or placeholder.\")\n",
        "    API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "BASE_URL = \"https://entropy.occybyte.com/api/eris/invoke\"\n",
        "# Use the same robust settings as weight init for consistency,\n",
        "# even though we only need a few bytes here.\n",
        "MAX_BYTES_PER_REQUEST = 16 * 1024 # 16384 bytes\n",
        "REQUEST_TIMEOUT = 15\n",
        "CHUNK_DELAY_SECONDS = 0.1\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY_SECONDS = 2\n",
        "\n",
        "NUM_BYTES_FOR_FLOAT_SAMPLING = 8 # Use 8 bytes (64 bits) for higher precision random float\n",
        "\n",
        "# --- Robust Quantum Byte Fetching (Required by fetch_quantum_float) ---\n",
        "# (Include the full fetch_quantum_bytes function from the weight init example here)\n",
        "def fetch_quantum_bytes(total_bytes_needed: int, api_key: str) -> bytes | None:\n",
        "    \"\"\"\n",
        "    Fetches raw quantum bytes from the ERIS API, handling chunking and retries.\n",
        "    (Same implementation as in the weight initialization example)\n",
        "    \"\"\"\n",
        "    if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"Error: API Key not configured.\")\n",
        "        return None\n",
        "\n",
        "    if total_bytes_needed <= 0:\n",
        "        return b''\n",
        "\n",
        "    all_fetched_bytes = bytearray()\n",
        "    # Calculate number of chunks based on the NEW max size\n",
        "    num_chunks = math.ceil(total_bytes_needed / MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "    print(f\"Starting fetch for {total_bytes_needed} bytes in {num_chunks} chunk(s) (max {MAX_BYTES_PER_REQUEST} bytes/chunk, {MAX_RETRIES} retries/chunk)...\")\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        bytes_to_request_this_chunk = min(total_bytes_needed - len(all_fetched_bytes), MAX_BYTES_PER_REQUEST)\n",
        "        if bytes_to_request_this_chunk <= 0: break # Already fetched enough\n",
        "\n",
        "        url = f\"{BASE_URL}?size={bytes_to_request_this_chunk}\"\n",
        "        headers = {\"X-API-Key\": api_key}\n",
        "\n",
        "        success = False\n",
        "        for attempt in range(MAX_RETRIES + 1):\n",
        "            try:\n",
        "                # Reduced print frequency for float fetching as it's small\n",
        "                if attempt == 0:\n",
        "                    print(f\"  Requesting chunk {i+1}/{num_chunks} ({bytes_to_request_this_chunk} bytes)...\")\n",
        "                else:\n",
        "                     print(f\"  Retrying chunk {i+1}, attempt {attempt+1}...\")\n",
        "\n",
        "                response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                json_response = response.json()\n",
        "                if \"data\" in json_response:\n",
        "                    base64_data = json_response[\"data\"]\n",
        "                    chunk_bytes = base64.b64decode(base64_data)\n",
        "                    if len(chunk_bytes) < bytes_to_request_this_chunk:\n",
        "                         raise ValueError(f\"API returned fewer bytes ({len(chunk_bytes)}) than requested ({bytes_to_request_this_chunk})\")\n",
        "\n",
        "                    all_fetched_bytes.extend(chunk_bytes)\n",
        "                    if attempt == 0: # Only print success on first try\n",
        "                         print(f\"  Received {len(chunk_bytes)} bytes.\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    raise ValueError(\"'data' field not found in API response\")\n",
        "            except Exception as e: # Catch all errors for retry logic\n",
        "                print(f\"  Attempt {attempt+1} failed: {e}\")\n",
        "                if attempt < MAX_RETRIES:\n",
        "                    print(f\"  Waiting {RETRY_DELAY_SECONDS}s before retrying...\")\n",
        "                    time.sleep(RETRY_DELAY_SECONDS)\n",
        "                else:\n",
        "                     print(f\"Chunk {i+1} failed after {MAX_RETRIES+1} attempts. Aborting fetch.\")\n",
        "                     return None\n",
        "\n",
        "        if not success: return None # Exit if a chunk failed permanently\n",
        "        if success and i < num_chunks - 1: time.sleep(CHUNK_DELAY_SECONDS)\n",
        "\n",
        "    if len(all_fetched_bytes) < total_bytes_needed:\n",
        "        print(f\"Error: Final fetched bytes ({len(all_fetched_bytes)}) less than required ({total_bytes_needed}).\")\n",
        "        return None\n",
        "\n",
        "    # No need to print total for small float requests\n",
        "    # print(f\"Successfully fetched {len(all_fetched_bytes)} total bytes (needed {total_bytes_needed}).\")\n",
        "    return bytes(all_fetched_bytes)\n",
        "\n",
        "\n",
        "# --- Helper Function: Fetch Quantum Float ---\n",
        "\n",
        "def fetch_quantum_float(num_bytes: int, api_key: str) -> float | None:\n",
        "    \"\"\"\n",
        "    Fetches quantum bytes using the robust fetcher and converts them\n",
        "    into a float in the range [0.0, 1.0).\n",
        "    \"\"\"\n",
        "    quantum_bytes = fetch_quantum_bytes(num_bytes, api_key) # Use the robust fetcher\n",
        "\n",
        "    if quantum_bytes is None:\n",
        "        return None # Error handled in fetch_quantum_bytes\n",
        "\n",
        "    # Slice to exact size needed, in case fetcher returned more (less likely for small requests)\n",
        "    exact_quantum_bytes = quantum_bytes[:num_bytes]\n",
        "    if len(exact_quantum_bytes) != num_bytes:\n",
        "         print(f\"Error: Sliced byte count ({len(exact_quantum_bytes)}) mismatch after fetch ({num_bytes} needed).\")\n",
        "         return None\n",
        "\n",
        "    try:\n",
        "        if num_bytes == 8:\n",
        "            random_int = struct.unpack('<Q', exact_quantum_bytes)[0] # <Q = little-endian unsigned 64-bit int\n",
        "            max_val = (1 << 64) - 1\n",
        "        elif num_bytes == 4:\n",
        "             random_int = struct.unpack('<I', exact_quantum_bytes)[0] # <I = little-endian unsigned 32-bit int\n",
        "             max_val = (1 << 32) - 1\n",
        "        else:\n",
        "             # Fallback for other sizes (less ideal distribution)\n",
        "             random_int = int.from_bytes(exact_quantum_bytes, byteorder='little', signed=False)\n",
        "             max_val = (1 << (num_bytes * 8)) - 1\n",
        "\n",
        "        # Normalize to [0.0, 1.0)\n",
        "        return float(random_int) / (max_val + 1)\n",
        "\n",
        "    except struct.error as e:\n",
        "         print(f\"Error unpacking bytes for float: {e}\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "         print(f\"Unexpected error converting bytes to float: {e}\")\n",
        "         return None\n",
        "\n",
        "\n",
        "# --- Quantum Sampling Function ---\n",
        "\n",
        "def quantum_enhanced_sampling(logits: torch.Tensor, api_key: str, temperature: float = 1.0) -> int | None:\n",
        "    \"\"\"\n",
        "    Samples a token index from logits using quantum randomness via inverse transform sampling.\n",
        "    \"\"\"\n",
        "    if temperature <= 0:\n",
        "        print(\"Warning: Temperature should be positive. Using temperature=1.0\")\n",
        "        temperature = 1.0\n",
        "\n",
        "    # Fetch a single high-precision quantum random float [0.0, 1.0)\n",
        "    quantum_rand_float = fetch_quantum_float(NUM_BYTES_FOR_FLOAT_SAMPLING, api_key)\n",
        "\n",
        "    if quantum_rand_float is None:\n",
        "        print(\"Failed to get quantum random number for sampling.\")\n",
        "        return None\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(probabilities, dim=-1)\n",
        "\n",
        "    # Add a small epsilon to handle potential float issues at boundaries\n",
        "    epsilon = 1e-9\n",
        "    sampled_index = torch.searchsorted(cumulative_probs,\n",
        "                                      torch.tensor([quantum_rand_float + epsilon], device=logits.device),\n",
        "                                      right=False)\n",
        "\n",
        "    return sampled_index.item()\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "try:\n",
        "    # Ensure transformers is installed: pip install transformers\n",
        "    import transformers\n",
        "\n",
        "    model_name = \"gpt2\"\n",
        "    print(f\"Loading model: {model_name}...\")\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "    if tokenizer.bos_token_id is not None:\n",
        "        input_ids = torch.tensor([[tokenizer.bos_token_id]])\n",
        "    else:\n",
        "         input_text = \"The\"\n",
        "         print(f\"Warning: BOS token not found for {model_name}. Starting sequence with '{input_text}'.\")\n",
        "         input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    print(f\"Input token IDs: {input_ids.tolist()}\")\n",
        "\n",
        "    print(\"Running model inference...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        next_token_logits = outputs.logits[0, -1, :]\n",
        "    print(f\"Logits shape for next token: {next_token_logits.shape}\")\n",
        "\n",
        "    print(f\"Sampling next token using quantum randomness (fetching {NUM_BYTES_FOR_FLOAT_SAMPLING} bytes)...\")\n",
        "    selected_token_id = quantum_enhanced_sampling(next_token_logits, API_KEY, temperature=0.8)\n",
        "\n",
        "    if selected_token_id is not None:\n",
        "        selected_token = tokenizer.decode([selected_token_id])\n",
        "        print(f\"Quantum-selected token ID: {selected_token_id}\")\n",
        "        print(f\"Quantum-selected token: '{selected_token}'\")\n",
        "    else:\n",
        "        print(\"Failed to sample token due to an error.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Error: Please install PyTorch and Transformers (`pip install torch transformers`)\")\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during model loading or inference: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HwoR2btRYpu"
      },
      "source": [
        "This is the AI Sampling example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeEuov_oRYzq"
      },
      "outputs": [],
      "source": [
        "# --- Quantum Choice Sampling Example ---\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import base64\n",
        "import math # For ceiling calculation\n",
        "import time # For delays\n",
        "\n",
        "# --- Configuration ---\n",
        "try:\n",
        "    # Use Colab secrets if available\n",
        "    from google.colab import userdata\n",
        "    API_KEY = userdata.get('OCCYBYTE_API_KEY')\n",
        "    if not API_KEY:\n",
        "        print(\"Warning: OCCYBYTE_API_KEY secret found but is empty. Falling back to env var or placeholder.\")\n",
        "        API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "except ImportError:\n",
        "    # Fallback for environments without google.colab\n",
        "    print(\"Warning: google.colab not found. Using OCCYBYTE_API_KEY environment variable or placeholder.\")\n",
        "    API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "BASE_URL = \"https://entropy.occybyte.com/api/eris/invoke\"\n",
        "# Use the same robust settings, even for potentially smaller requests\n",
        "MAX_BYTES_PER_REQUEST = 16 * 1024 # 16384 bytes\n",
        "REQUEST_TIMEOUT = 15\n",
        "CHUNK_DELAY_SECONDS = 0.1\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY_SECONDS = 2\n",
        "\n",
        "# --- Robust Quantum Byte Fetching (with Chunking, Retries, and Small Request Workaround) ---\n",
        "def fetch_quantum_bytes(total_bytes_needed: int, api_key: str) -> bytes | None:\n",
        "    \"\"\"\n",
        "    Fetches raw quantum bytes from the ERIS API, handling chunking, retries,\n",
        "    and adding a workaround for potential API base64 padding errors on small requests.\n",
        "    \"\"\"\n",
        "    if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"Error: API Key not configured.\")\n",
        "        return None\n",
        "\n",
        "    if total_bytes_needed <= 0:\n",
        "        return b''\n",
        "\n",
        "    # --- Workaround: Define a minimum request size known to work ---\n",
        "    # We saw it return 24 bytes for 16, and 12 bytes for 8. Let's try 16 or 24 as a minimum.\n",
        "    # Using 24 seems safer as it's a multiple of 3 (good for base64) and we saw it returned.\n",
        "    MIN_REQUEST_SIZE_WORKAROUND = 24\n",
        "    bytes_to_actually_request = max(total_bytes_needed, MIN_REQUEST_SIZE_WORKAROUND) \\\n",
        "                                    if total_bytes_needed < MAX_BYTES_PER_REQUEST else total_bytes_needed\n",
        "    # Only apply workaround if the total needed is small AND fits within one chunk.\n",
        "    # If total_bytes_needed is large, chunking takes over anyway.\n",
        "\n",
        "    if bytes_to_actually_request > total_bytes_needed:\n",
        "         print(f\"Workaround: Adjusted request size from {total_bytes_needed} to {bytes_to_actually_request} to potentially avoid API padding errors.\")\n",
        "\n",
        "    all_fetched_bytes = bytearray()\n",
        "    # Calculate chunks based on the potentially adjusted request size\n",
        "    num_chunks = math.ceil(bytes_to_actually_request / MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "    print(f\"Starting fetch for {total_bytes_needed} bytes (requesting {bytes_to_actually_request}) in {num_chunks} chunk(s)...\")\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        # Calculate based on bytes_to_actually_request\n",
        "        bytes_to_request_this_chunk = min(bytes_to_actually_request - len(all_fetched_bytes), MAX_BYTES_PER_REQUEST)\n",
        "        if bytes_to_request_this_chunk <= 0: break\n",
        "\n",
        "        url = f\"{BASE_URL}?size={bytes_to_request_this_chunk}\"\n",
        "        headers = {\"X-API-Key\": api_key}\n",
        "\n",
        "        success = False\n",
        "        for attempt in range(MAX_RETRIES + 1):\n",
        "            try:\n",
        "                print(f\"  Requesting chunk {i+1}/{num_chunks} ({bytes_to_request_this_chunk} bytes), attempt {attempt+1}/{MAX_RETRIES+1}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                json_response = response.json()\n",
        "                if \"data\" in json_response:\n",
        "                    base64_data = json_response[\"data\"]\n",
        "                    # >>> Try decoding here to catch padding error early <<<\n",
        "                    try:\n",
        "                         chunk_bytes = base64.b64decode(base64_data)\n",
        "                    except base64.binascii.Error as decode_error:\n",
        "                         # Re-raise as a ValueError to be caught by the retry loop's general exception handler\n",
        "                         raise ValueError(f\"Base64 decode failed: {decode_error}. API likely returned malformed data for size {bytes_to_request_this_chunk}.\")\n",
        "\n",
        "                    # Check if API returned *at least* what we asked for (it might return more)\n",
        "                    # This check might be less critical now if the decode succeeded, but keep for safety\n",
        "                    if len(chunk_bytes) < bytes_to_request_this_chunk:\n",
        "                         raise ValueError(f\"API returned fewer bytes ({len(chunk_bytes)}) than requested ({bytes_to_request_this_chunk}) after successful decode.\")\n",
        "\n",
        "                    all_fetched_bytes.extend(chunk_bytes)\n",
        "                    print(f\"  Received {len(chunk_bytes)} bytes. Total fetched so far: {len(all_fetched_bytes)}\")\n",
        "                    success = True\n",
        "                    break # Exit retry loop on success\n",
        "                else:\n",
        "                    raise ValueError(\"'data' field not found in API response\")\n",
        "            except Exception as e: # Catches network errors, status errors, ValueErrors from above\n",
        "                print(f\"  Attempt {attempt+1} failed: {e}\")\n",
        "                if attempt < MAX_RETRIES:\n",
        "                    print(f\"  Waiting {RETRY_DELAY_SECONDS}s before retrying...\")\n",
        "                    time.sleep(RETRY_DELAY_SECONDS)\n",
        "                else:\n",
        "                     print(f\"Chunk {i+1} failed after {MAX_RETRIES+1} attempts. Aborting fetch.\")\n",
        "                     return None # Failed to get this chunk after all retries\n",
        "\n",
        "        if not success: return None # Exit loop if a chunk failed permanently\n",
        "        if success and i < num_chunks - 1: time.sleep(CHUNK_DELAY_SECONDS)\n",
        "\n",
        "    # --- Check final byte count against ORIGINAL needed amount ---\n",
        "    if len(all_fetched_bytes) < total_bytes_needed:\n",
        "        print(f\"Error: Final fetched bytes ({len(all_fetched_bytes)}) less than originally required ({total_bytes_needed}).\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Successfully fetched {len(all_fetched_bytes)} total bytes (originally needed {total_bytes_needed}).\")\n",
        "    # Return the fetched bytes (potentially more than originally needed, will be sliced later)\n",
        "    return bytes(all_fetched_bytes)\n",
        "\n",
        "\n",
        "# --- Quantum Choice Sampling Function ---\n",
        "def quantum_choice_sampler(options: list, num_samples: int, api_key: str) -> list | None:\n",
        "    \"\"\"\n",
        "    Selects items from a list using quantum random bytes fetched via API.\n",
        "    Note: Uses simple modulo mapping, which may introduce slight bias\n",
        "          if len(options) does not evenly divide 256.\n",
        "\n",
        "    Args:\n",
        "        options: The list of items to choose from.\n",
        "        num_samples: The number of samples to generate (fetches this many bytes).\n",
        "        api_key: Your Occybyte API key.\n",
        "\n",
        "    Returns:\n",
        "        A list containing num_samples items selected from options,\n",
        "        or None if an error occurred during fetching.\n",
        "    \"\"\"\n",
        "    if not options:\n",
        "        print(\"Error: Options list cannot be empty.\")\n",
        "        return None\n",
        "    if num_samples <= 0:\n",
        "        print(\"Error: Number of samples must be positive.\")\n",
        "        return None\n",
        "\n",
        "    # Fetch exactly num_samples bytes using the robust fetcher\n",
        "    all_quantum_bytes = fetch_quantum_bytes(num_samples, api_key)\n",
        "\n",
        "    if all_quantum_bytes is None:\n",
        "        print(\"Failed to fetch quantum bytes for sampling.\")\n",
        "        return None # Error message handled in fetch_quantum_bytes\n",
        "\n",
        "    # --- Slice the received bytes to EXACTLY the amount needed ---\n",
        "    # Handles cases where the API returned more bytes than requested.\n",
        "    exact_quantum_bytes = all_quantum_bytes[:num_samples]\n",
        "\n",
        "    if len(exact_quantum_bytes) != num_samples:\n",
        "        print(f\"Error: After slicing, byte count ({len(exact_quantum_bytes)}) doesn't match required ({num_samples}).\")\n",
        "        return Nonea\n",
        "\n",
        "    num_options = len(options)\n",
        "    # Use list comprehension for a functional style with the correctly sized byte list\n",
        "    selected_indices = [byte % num_options for byte in exact_quantum_bytes]\n",
        "\n",
        "    return [options[i] for i in selected_indices]\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "potential_next_tokens = [\"the\", \"a\", \"one\", \"this\", \"some\", \"quantum\", \"random\", \"choice\", \"entropy\", \"sample\"]\n",
        "num_choices = 15 # How many tokens we want to select\n",
        "\n",
        "print(f\"\\nAttempting to select {num_choices} items using quantum randomness...\")\n",
        "selected_tokens = quantum_choice_sampler(potential_next_tokens, num_choices, API_KEY)\n",
        "\n",
        "if selected_tokens:\n",
        "    print(f\"Quantum-selected items ({len(selected_tokens)}): {selected_tokens}\")\n",
        "else:\n",
        "    print(\"Failed to select items due to an error during data fetch.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1uVaQ9KiXKl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQTNS2giW1c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import base64\n",
        "import math # For ceiling calculation\n",
        "import time # For delays\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# --- Configuration & API Key Handling ---\n",
        "try:\n",
        "    # Use Colab secrets if available\n",
        "    from google.colab import userdata\n",
        "    OCCYBYTE_API_KEY = userdata.get('OCCYBYTE_API_KEY')\n",
        "    HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY') # For Hugging Face Hub, if needed\n",
        "\n",
        "    if not OCCYBYTE_API_KEY:\n",
        "        print(\"Warning: OCCYBYTE_API_KEY secret found but is empty. Falling back to env var or placeholder.\")\n",
        "        OCCYBYTE_API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_OCCYBYTE_API_KEY_HERE\")\n",
        "    if not HUGGINGFACE_API_KEY:\n",
        "        print(\"Info: HUGGINGFACE_API_KEY secret found but is empty or not set. Using env var or proceeding without it (public models might not need it).\")\n",
        "        HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\") # Can be None\n",
        "\n",
        "except ImportError:\n",
        "    # Fallback for environments without google.colab\n",
        "    print(\"Warning: google.colab not found. Using environment variables or placeholders for API keys.\")\n",
        "    OCCYBYTE_API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_OCCYBYTE_API_KEY_HERE\")\n",
        "    HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\") # Can be None\n",
        "\n",
        "if OCCYBYTE_API_KEY == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "    print(\"CRITICAL WARNING: Please set your OCCYBYTE_API_KEY in Colab secrets or as an environment variable.\")\n",
        "if HUGGINGFACE_API_KEY:\n",
        "    print(f\"Hugging Face API Key found (last 4 chars for verification if needed: ...{HUGGINGFACE_API_KEY[-4:] if len(HUGGINGFACE_API_KEY) > 4 else '****'})\")\n",
        "else:\n",
        "    print(\"Info: Hugging Face API Key not actively used in this script for public model loading, but fetched if provided.\")\n",
        "\n",
        "# ERIS API Configuration\n",
        "ERIS_BASE_URL = \"https://entropy.occybyte.com/api/eris/invoke\"\n",
        "ERIS_MAX_BYTES_PER_REQUEST = 16 * 1024 # 16384 bytes\n",
        "ERIS_REQUEST_TIMEOUT = 15\n",
        "ERIS_CHUNK_DELAY_SECONDS = 0.1\n",
        "ERIS_MAX_RETRIES = 3\n",
        "ERIS_RETRY_DELAY_SECONDS = 2\n",
        "ERIS_MIN_REQUEST_SIZE_WORKAROUND = 24 # As per your tested value\n",
        "\n",
        "\n",
        "# --- Configuration for Reproducible ERIS Seeds (ERIS HEX SEED)---\n",
        "# Set USE_SAVED_ERIS_SEED to True to use a pre-captured seed for the ERIS arm,\n",
        "# or False to fetch a new one from the ERIS API.\n",
        "USE_SAVED_ERIS_SEED = False  # << CHANGE THIS TO True TO USE A SAVED SEED\n",
        "# If USE_SAVED_ERIS_SEED is True, provide ONE of the following:\n",
        "SAVED_ERIS_HEX_SEED = None # Example: \"66cd5665\" (a hex string you previously saved)\n",
        "SAVED_ERIS_INT_SEED = None # Example: 1724733029 (an integer seed you previously saved)\n",
        "\n",
        "# --- Helper function to manage experiment seeding ---\n",
        "def get_experiment_seed(api_key: str, use_saved: bool, saved_hex: str = None, saved_int: int = None, num_bytes_for_new_seed: int = 4) -> int | None:\n",
        "    \"\"\"\n",
        "    Gets a seed for the experiment.\n",
        "    If use_saved is True, it attempts to use a provided saved seed.\n",
        "    Otherwise, it fetches a new seed from ERIS.\n",
        "    \"\"\"\n",
        "    if use_saved:\n",
        "        if saved_hex:\n",
        "            try:\n",
        "                saved_bytes = bytes.fromhex(saved_hex)\n",
        "                seed_to_use = int.from_bytes(saved_bytes, 'big')\n",
        "                print(f\"Using SAVED ERIS hex seed: {saved_hex}, Converted to Integer seed: {seed_to_use}\")\n",
        "                return seed_to_use\n",
        "            except ValueError:\n",
        "                print(f\"Error: Invalid saved_hex_seed: '{saved_hex}'. Could not convert from hex.\")\n",
        "                return None\n",
        "        elif saved_int is not None:\n",
        "            print(f\"Using SAVED ERIS integer seed: {saved_int}\")\n",
        "            return saved_int\n",
        "        else:\n",
        "            print(\"Error: USE_SAVED_ERIS_SEED is True, but no SAVED_ERIS_HEX_SEED or SAVED_ERIS_INT_SEED was provided.\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Fetching NEW seed from ERIS for the experiment...\")\n",
        "        # Assuming fetch_seed_from_eris takes num_bytes as an argument\n",
        "        return fetch_seed_from_eris(api_key, num_bytes=num_bytes_for_new_seed)\n",
        "\n",
        "# --- ERIS API Fetching Logic (Your Robust Functions) ---\n",
        "def fetch_quantum_bytes(total_bytes_needed: int, api_key: str) -> bytes | None:\n",
        "    \"\"\"\n",
        "    Fetches raw quantum bytes from the ERIS API, handling chunking, retries,\n",
        "    and adding a workaround for potential API base64 padding errors on small requests.\n",
        "    \"\"\"\n",
        "    # --- ACCESS GLOBAL CONSTANTS ---\n",
        "    # These are defined outside the function but used here.\n",
        "    # No 'global' keyword needed for reading them.\n",
        "    # ERIS_BASE_URL, ERIS_MAX_BYTES_PER_REQUEST, ERIS_REQUEST_TIMEOUT, etc.\n",
        "    # ERIS_MIN_REQUEST_SIZE_WORKAROUND\n",
        "\n",
        "    if not api_key or api_key == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        print(\"Error: Occybyte API Key not configured for fetch_quantum_bytes.\")\n",
        "        return None\n",
        "\n",
        "    if total_bytes_needed <= 0:\n",
        "        return b''\n",
        "\n",
        "    bytes_to_actually_request = total_bytes_needed\n",
        "    # CORRECTED LINE: Use the globally defined ERIS_MAX_BYTES_PER_REQUEST\n",
        "    if total_bytes_needed < ERIS_MAX_BYTES_PER_REQUEST and total_bytes_needed < ERIS_MIN_REQUEST_SIZE_WORKAROUND:\n",
        "        bytes_to_actually_request = ERIS_MIN_REQUEST_SIZE_WORKAROUND\n",
        "        print(f\"Workaround: Adjusted request size from {total_bytes_needed} to {bytes_to_actually_request} to potentially avoid API padding errors for small requests.\")\n",
        "\n",
        "    all_fetched_bytes = bytearray()\n",
        "    # CORRECTED LINE: Use the globally defined ERIS_MAX_BYTES_PER_REQUEST\n",
        "    num_chunks = math.ceil(bytes_to_actually_request / ERIS_MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "\n",
        "    print(f\"Starting ERIS fetch for {total_bytes_needed} original bytes (requesting {bytes_to_actually_request} potentially due to workaround) in {num_chunks} chunk(s)...\")\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        bytes_remaining_to_request_overall = bytes_to_actually_request - len(all_fetched_bytes)\n",
        "        # CORRECTED LINE: Use the globally defined ERIS_MAX_BYTES_PER_REQUEST\n",
        "        bytes_to_request_this_chunk = min(bytes_remaining_to_request_overall, ERIS_MAX_BYTES_PER_REQUEST)\n",
        "\n",
        "\n",
        "        if bytes_to_request_this_chunk <= 0: break\n",
        "\n",
        "        # CORRECTED LINE: Use the globally defined ERIS_BASE_URL\n",
        "        url = f\"{ERIS_BASE_URL}?size={bytes_to_request_this_chunk}\"\n",
        "        headers = {\"X-API-Key\": api_key}\n",
        "        success = False\n",
        "\n",
        "        for attempt in range(ERIS_MAX_RETRIES + 1): # Use globally defined ERIS_MAX_RETRIES\n",
        "            try:\n",
        "                print(f\"  Requesting ERIS chunk {i+1}/{num_chunks} ({bytes_to_request_this_chunk} bytes), attempt {attempt+1}/{ERIS_MAX_RETRIES+1}...\")\n",
        "                 # Use globally defined ERIS_REQUEST_TIMEOUT\n",
        "                response = requests.get(url, headers=headers, timeout=ERIS_REQUEST_TIMEOUT)\n",
        "                response.raise_for_status()\n",
        "                json_response = response.json()\n",
        "\n",
        "                if \"data\" in json_response:\n",
        "                    base64_data = json_response[\"data\"]\n",
        "                    try:\n",
        "                         chunk_bytes = base64.b64decode(base64_data)\n",
        "                    except base64.binascii.Error as decode_error:\n",
        "                         raise ValueError(f\"Base64 decode failed: {decode_error}. API likely returned malformed data for size {bytes_to_request_this_chunk}.\")\n",
        "\n",
        "                    all_fetched_bytes.extend(chunk_bytes)\n",
        "                    print(f\"  Received {len(chunk_bytes)} bytes from ERIS. Total fetched so far: {len(all_fetched_bytes)}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    raise ValueError(\"'data' field not found in ERIS API response\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ERIS Attempt {attempt+1} failed: {e}\")\n",
        "                if attempt < ERIS_MAX_RETRIES: # Use globally defined ERIS_MAX_RETRIES\n",
        "                    # Use globally defined ERIS_RETRY_DELAY_SECONDS\n",
        "                    print(f\"  Waiting {ERIS_RETRY_DELAY_SECONDS}s before retrying ERIS...\")\n",
        "                    time.sleep(ERIS_RETRY_DELAY_SECONDS)\n",
        "                else:\n",
        "                     print(f\"ERIS Chunk {i+1} failed after {ERIS_MAX_RETRIES+1} attempts. Aborting fetch.\")\n",
        "                     return None\n",
        "        if not success: return None\n",
        "        # Use globally defined ERIS_CHUNK_DELAY_SECONDS\n",
        "        if success and i < num_chunks - 1: time.sleep(ERIS_CHUNK_DELAY_SECONDS)\n",
        "\n",
        "\n",
        "    if len(all_fetched_bytes) < total_bytes_needed:\n",
        "        print(f\"Warning: Final ERIS fetched bytes ({len(all_fetched_bytes)}) less than originally required ({total_bytes_needed}). This might be okay if API returned slightly less than workaround request but still >= original.\")\n",
        "        if len(all_fetched_bytes) < total_bytes_needed:\n",
        "             print(f\"Error: Critical shortage. Fetched {len(all_fetched_bytes)}, needed {total_bytes_needed}.\")\n",
        "             return None\n",
        "\n",
        "    print(f\"Successfully fetched {len(all_fetched_bytes)} total bytes from ERIS (originally needed {total_bytes_needed}).\")\n",
        "    return bytes(all_fetched_bytes[:total_bytes_needed])\n",
        "\n",
        "\n",
        "def fetch_seed_from_eris(api_key: str, num_bytes: int = 4) -> int | None:\n",
        "    \"\"\"Fetches a specified number of quantum bytes from ERIS and converts them to an integer seed.\"\"\"\n",
        "    print(f\"Fetching {num_bytes} bytes from ERIS for seed generation...\")\n",
        "    seed_bytes = fetch_quantum_bytes(num_bytes, api_key)\n",
        "    if seed_bytes:\n",
        "        seed_int = int.from_bytes(seed_bytes, 'big') # Or 'little', ensure consistency\n",
        "        print(f\"ERIS seed bytes: {seed_bytes.hex()}, Integer seed: {seed_int}\")\n",
        "        return seed_int\n",
        "    print(\"Failed to fetch seed from ERIS.\")\n",
        "    return None\n",
        "\n",
        "# --- Quantum Choice Sampling Function (Included as per your example, though not directly used for seeding generate()) ---\n",
        "def quantum_choice_sampler(options: list, num_samples: int, api_key: str) -> list | None:\n",
        "    \"\"\"\n",
        "    Selects items from a list using quantum random bytes fetched via API.\n",
        "    Note: Uses simple modulo mapping, which may introduce slight bias\n",
        "          if len(options) does not evenly divide 256.\n",
        "    \"\"\"\n",
        "    if not options:\n",
        "        print(\"Error: Options list cannot be empty for quantum_choice_sampler.\")\n",
        "        return None\n",
        "    if num_samples <= 0:\n",
        "        print(\"Error: Number of samples must be positive for quantum_choice_sampler.\")\n",
        "        return None\n",
        "\n",
        "    all_quantum_bytes = fetch_quantum_bytes(num_samples, api_key)\n",
        "    if all_quantum_bytes is None:\n",
        "        print(\"Failed to fetch quantum bytes for sampling.\")\n",
        "        return None\n",
        "\n",
        "    if len(all_quantum_bytes) != num_samples: # Should be handled by fetch_quantum_bytes now\n",
        "        print(f\"Error: Byte count mismatch in quantum_choice_sampler. Expected {num_samples}, got {len(all_quantum_bytes)}.\")\n",
        "        return None\n",
        "\n",
        "    num_options = len(options)\n",
        "    selected_indices = [byte % num_options for byte in all_quantum_bytes]\n",
        "    return [options[i] for i in selected_indices]\n",
        "\n",
        "# --- LLM Configuration & Experiment ---\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # You can change this to other models like \"distilgpt2\", \"EleutherAI/pythia-70m\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", gpt2\n",
        "PRNG_FIXED_SEED = 42\n",
        "NUM_BYTES_FOR_ERIS_SEED = 4 # Standard integer size for many PRNG seeds\n",
        "\n",
        "# Generation parameters (keep consistent for fair comparison)\n",
        "MAX_NEW_TOKENS = 200\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K = 50\n",
        "DO_SAMPLE = True # Important to actually use the randomness\n",
        "\n",
        "PROMPTS = [\n",
        "    \"The ancient prophecy spoke of a dragon that slept under the mountain, but it failed to mention\",\n",
        "    \"Explain the concept of recursion to a child using a story about a friendly robot.\",\n",
        "    \"Write a short poem about the silence of a winter forest.\",\n",
        "    \"What if the Roman Empire had access to basic steam power?\",\n",
        "    \"Generate a list of three unusual ingredients for a pizza.\"\n",
        "]\n",
        "\n",
        "# --- Main Experiment Logic ---\n",
        "def run_llm_experiment():\n",
        "    if OCCYBYTE_API_KEY == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        print(\"Cannot run experiment: Occybyte API Key is not set. Please configure it.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading LLM: {MODEL_NAME}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HUGGINGFACE_API_KEY)\n",
        "        # --- Updated Model Loading for Quantization (Example for Mistral-7B, adapt as needed) ---\n",
        "        if \"mistral\" in MODEL_NAME.lower() or \"mixtral\" in MODEL_NAME.lower() : # Basic check, refine if needed\n",
        "            print(f\"Applying 8-bit quantization for {MODEL_NAME} using bitsandbytes...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_NAME,\n",
        "                token=HUGGINGFACE_API_KEY,\n",
        "                load_in_8bit=True, # Enable 8-bit quantization\n",
        "                device_map=\"auto\"  # Automatically distribute model on available GPUs\n",
        "            )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=HUGGINGFACE_API_KEY)\n",
        "            # Move model to device if GPU available (standard models)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "\n",
        "\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            print(\"Tokenizer pad_token_id is None, setting to eos_token_id.\")\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        # Model's pad_token_id is usually handled by AutoModelForCausalLM if config is correct\n",
        "        # but good to be aware if issues arise with padding during generation.\n",
        "\n",
        "        # Determine device (especially for non-quantized models or for inputs)\n",
        "        # For quantized models with device_map=\"auto\", model.device might point to 'meta' or a specific GPU\n",
        "        # Inputs should still be moved to the GPU where computation happens if possible.\n",
        "        # A simple approach if not using device_map or if wanting to ensure inputs are on a specific device:\n",
        "        if hasattr(model, 'device') and model.device.type != 'meta':\n",
        "             device = model.device # if model is already on a device (e.g. after .to(device) or device_map)\n",
        "        else:\n",
        "             device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "             if not (\"mistral\" in MODEL_NAME.lower() or \"mixtral\" in MODEL_NAME.lower()): # if not using device_map for quantization\n",
        "                 model.to(device) # ensure model is on device if not auto-mapped\n",
        "\n",
        "        print(f\"LLM {MODEL_NAME} loaded. Effective device for inputs/manual ops: {device}. Model may be auto-mapped if quantized.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM or Tokenizer: {e}\")\n",
        "        print(\"If this is a private or gated model, ensure your HUGGINGFACE_API_KEY is correctly set and has access.\")\n",
        "        print(\"If using quantization, ensure 'bitsandbytes' and 'accelerate' are installed and CUDA is available.\")\n",
        "        return\n",
        "\n",
        "    for i, prompt_text in enumerate(PROMPTS):\n",
        "        print(f\"\\n--- Experiment for Prompt {i+1}/{len(PROMPTS)} ---\")\n",
        "        print(f\"Prompt: \\\"{prompt_text}\\\"\")\n",
        "\n",
        "        common_generation_args = {\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"top_k\": TOP_K,\n",
        "            \"do_sample\": DO_SAMPLE,\n",
        "            \"pad_token_id\": tokenizer.pad_token_id # Use tokenizer's pad_token_id for consistency\n",
        "        }\n",
        "\n",
        "        # Prepare inputs once for both ERIS and PRNG generation for this prompt\n",
        "        # Using tokenizer's max_length for truncation if defined, otherwise a large number.\n",
        "        # max_length for tokenizer should be model's max sequence length.\n",
        "        tokenizer_max_len = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length < 100000 else 512 # Fallback\n",
        "        # print(f\"Using tokenizer_max_len: {tokenizer_max_len} for prompt tokenization.\")\n",
        "\n",
        "        tokenized_inputs = tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True, # Pad to longest in batch (or max_length if specified here)\n",
        "            truncation=True, # Truncate to tokenizer_max_len\n",
        "            max_length=tokenizer_max_len # Ensure truncation respects model limits\n",
        "        )\n",
        "        input_ids = tokenized_inputs.input_ids.to(device)\n",
        "        attention_mask = tokenized_inputs.attention_mask.to(device)\n",
        "\n",
        "\n",
        "        # --- ERIS Seeded Generation (or Saved ERIS Seed) ---\n",
        "        print(\"\\nGenerating with ERIS-derived Seed (or saved ERIS seed)...\")\n",
        "        # Get the seed for this ERIS run (either new or saved)\n",
        "        current_eris_seed = get_experiment_seed(\n",
        "            OCCYBYTE_API_KEY,\n",
        "            USE_SAVED_ERIS_SEED,\n",
        "            saved_hex=SAVED_ERIS_HEX_SEED,\n",
        "            saved_int=SAVED_ERIS_INT_SEED,\n",
        "            num_bytes_for_new_seed=NUM_BYTES_FOR_ERIS_SEED\n",
        "        )\n",
        "\n",
        "        if current_eris_seed is not None:\n",
        "            try:\n",
        "                print(f\"  Setting global torch seed to ERIS-derived value: {current_eris_seed}\")\n",
        "                torch.manual_seed(current_eris_seed)\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.manual_seed_all(current_eris_seed)\n",
        "\n",
        "                eris_output_ids = model.generate(\n",
        "                    input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    **common_generation_args\n",
        "                )\n",
        "                eris_generated_text = tokenizer.decode(eris_output_ids[0, input_ids.shape[1]:], skip_special_tokens=True) # Decode only new tokens\n",
        "                print(f\"ERIS Output:\\n{eris_generated_text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during ERIS seeded generation: {e}\")\n",
        "        else:\n",
        "            print(\"Could not generate with ERIS seed as seed could not be obtained/derived.\")\n",
        "\n",
        "        # --- PRNG Seeded Generation ---\n",
        "        print(\"\\nGenerating with PRNG Fixed Seed...\")\n",
        "        try:\n",
        "            print(f\"  Setting global torch seed to PRNG fixed: {PRNG_FIXED_SEED}\")\n",
        "            torch.manual_seed(PRNG_FIXED_SEED)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(PRNG_FIXED_SEED)\n",
        "\n",
        "            prng_output_ids = model.generate(\n",
        "                input_ids, # Use the same input_ids and attention_mask\n",
        "                attention_mask=attention_mask,\n",
        "                **common_generation_args\n",
        "            )\n",
        "            prng_generated_text = tokenizer.decode(prng_output_ids[0, input_ids.shape[1]:], skip_special_tokens=True) # Decode only new tokens\n",
        "            print(f\"PRNG Output:\\n{prng_generated_text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during PRNG seeded generation: {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting LLM Hallucination Experiment with ERIS vs PRNG...\")\n",
        "    run_llm_experiment()\n",
        "\n",
        "    # Example of using your quantum_choice_sampler (not tied to LLM here, just to show it runs)\n",
        "    print(\"\\n--- Example of quantum_choice_sampler (separate from LLM demo) ---\")\n",
        "    if OCCYBYTE_API_KEY != \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        potential_tokens = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\"]\n",
        "        num_choices_to_make = 5\n",
        "        print(f\"Attempting to select {num_choices_to_make} items from {potential_tokens} using ERIS quantum randomness...\")\n",
        "        selected_items = quantum_choice_sampler(potential_tokens, num_choices_to_make, OCCYBYTE_API_KEY)\n",
        "        if selected_items:\n",
        "            print(f\"Quantum-selected items ({len(selected_items)}): {selected_items}\")\n",
        "        else:\n",
        "            print(\"Failed to select items using quantum_choice_sampler.\")\n",
        "    else:\n",
        "        print(\"Skipping quantum_choice_sampler example as Occybyte API key is not set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5_B1u_L9HvD"
      },
      "source": [
        "The bits and bytes version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN-IpvZ99JQr"
      },
      "outputs": [],
      "source": [
        "# --- Essential Library Installation ---\n",
        "# Best to run this in a separate cell at the very top of your Colab notebook.\n",
        "# After running this, YOU MUST RESTART THE RUNTIME if bitsandbytes or accelerate were newly installed or updated.\n",
        "# Go to \"Runtime\" -> \"Restart runtime\" in the Colab menu.\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    import accelerate\n",
        "    import transformers\n",
        "    print(f\"Transformers version: {transformers.__version__}\")\n",
        "    print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "    print(f\"BitsAndBytes version: {bitsandbytes.__version__}\")\n",
        "    print(\"Required libraries are already available.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Missing one or more libraries ({e}). Installing...\")\n",
        "    # In Colab, use !pip install.\n",
        "    if \"google.colab\" in str(get_ipython()): # type: ignore\n",
        "        get_ipython().system('pip install transformers accelerate bitsandbytes --upgrade') # type: ignore\n",
        "        print(\"Installation complete. IMPORTANT: PLEASE RESTART THE RUNTIME NOW (Runtime -> Restart runtime).\")\n",
        "        # Exit to force user to restart and re-run for new libraries to take effect.\n",
        "        # This is a bit aggressive but helps avoid issues with bitsandbytes not finding CUDA.\n",
        "        import os\n",
        "        os._exit(0)\n",
        "    else:\n",
        "        print(\"Please install missing libraries manually: pip install transformers accelerate bitsandbytes --upgrade\")\n",
        "        import sys\n",
        "        sys.exit(\"Missing required libraries for non-Colab environment.\")\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import base64\n",
        "import math # For ceiling calculation\n",
        "import time # For delays\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # Import BitsAndBytesConfig\n",
        "\n",
        "# --- Configuration & API Key Handling ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OCCYBYTE_API_KEY = userdata.get('OCCYBYTE_API_KEY')\n",
        "    HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
        "    if not OCCYBYTE_API_KEY:\n",
        "        print(\"Warning: OCCYBYTE_API_KEY secret found but is empty. Falling back to env var or placeholder.\")\n",
        "        OCCYBYTE_API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_OCCYBYTE_API_KEY_HERE\")\n",
        "    if not HUGGINGFACE_API_KEY:\n",
        "        print(\"Info: HUGGINGFACE_API_KEY secret not found or empty. Using env var or proceeding without it.\")\n",
        "        HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "except ImportError:\n",
        "    print(\"Warning: google.colab not found. Using environment variables for API keys.\")\n",
        "    OCCYBYTE_API_KEY = os.getenv(\"OCCYBYTE_API_KEY\", \"YOUR_OCCYBYTE_API_KEY_HERE\")\n",
        "    HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "if OCCYBYTE_API_KEY == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "    print(\"CRITICAL WARNING: Please set your OCCYBYTE_API_KEY in Colab secrets or as an environment variable.\")\n",
        "if HUGGINGFACE_API_KEY:\n",
        "    print(f\"Hugging Face API Key found (Token for model access).\")\n",
        "else:\n",
        "    print(\"Info: Hugging Face API Key not found. Access to gated models might fail.\")\n",
        "\n",
        "# ERIS API Configuration (remains the same)\n",
        "ERIS_BASE_URL = \"https://entropy.occybyte.com/api/eris/invoke\"\n",
        "ERIS_MAX_BYTES_PER_REQUEST = 16 * 1024\n",
        "ERIS_REQUEST_TIMEOUT = 15\n",
        "ERIS_CHUNK_DELAY_SECONDS = 0.1\n",
        "ERIS_MAX_RETRIES = 3\n",
        "ERIS_RETRY_DELAY_SECONDS = 2\n",
        "ERIS_MIN_REQUEST_SIZE_WORKAROUND = 24\n",
        "\n",
        "# --- ERIS API Fetching Logic (remains the same) ---\n",
        "def fetch_quantum_bytes(total_bytes_needed: int, api_key: str) -> bytes | None:\n",
        "    if not api_key or api_key == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        print(\"Error: Occybyte API Key not configured for fetch_quantum_bytes.\")\n",
        "        return None\n",
        "    if total_bytes_needed <= 0: return b''\n",
        "    bytes_to_actually_request = total_bytes_needed\n",
        "    if total_bytes_needed < ERIS_MAX_BYTES_PER_REQUEST and total_bytes_needed < ERIS_MIN_REQUEST_SIZE_WORKAROUND:\n",
        "        bytes_to_actually_request = ERIS_MIN_REQUEST_SIZE_WORKAROUND\n",
        "        print(f\"Workaround: Adjusted request size from {total_bytes_needed} to {bytes_to_actually_request} for ERIS.\")\n",
        "    all_fetched_bytes = bytearray()\n",
        "    num_chunks = math.ceil(bytes_to_actually_request / ERIS_MAX_BYTES_PER_REQUEST)\n",
        "    # print(f\"Starting ERIS fetch for {total_bytes_needed} original bytes (requesting {bytes_to_actually_request}) in {num_chunks} chunk(s)...\")\n",
        "    for i in range(num_chunks):\n",
        "        bytes_remaining_to_request_overall = bytes_to_actually_request - len(all_fetched_bytes)\n",
        "        bytes_to_request_this_chunk = min(bytes_remaining_to_request_overall, ERIS_MAX_BYTES_PER_REQUEST)\n",
        "        if bytes_to_request_this_chunk <= 0: break\n",
        "        url = f\"{ERIS_BASE_URL}?size={bytes_to_request_this_chunk}\"\n",
        "        headers = {\"X-API-Key\": api_key}\n",
        "        success = False\n",
        "        for attempt in range(ERIS_MAX_RETRIES + 1):\n",
        "            try:\n",
        "                # print(f\"  Requesting ERIS chunk {i+1}/{num_chunks} ({bytes_to_request_this_chunk} bytes), attempt {attempt+1}/{ERIS_MAX_RETRIES+1}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=ERIS_REQUEST_TIMEOUT)\n",
        "                response.raise_for_status()\n",
        "                json_response = response.json()\n",
        "                if \"data\" in json_response:\n",
        "                    base64_data = json_response[\"data\"]\n",
        "                    try: chunk_bytes = base64.b64decode(base64_data)\n",
        "                    except base64.binascii.Error as decode_error: raise ValueError(f\"Base64 decode failed: {decode_error}.\")\n",
        "                    all_fetched_bytes.extend(chunk_bytes)\n",
        "                    # print(f\"  Received {len(chunk_bytes)} bytes from ERIS. Total: {len(all_fetched_bytes)}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else: raise ValueError(\"'data' field not found in ERIS API response\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ERIS Attempt {attempt+1} failed for chunk {i+1}: {e}\")\n",
        "                if attempt < ERIS_MAX_RETRIES: time.sleep(ERIS_RETRY_DELAY_SECONDS)\n",
        "                else: print(f\"ERIS Chunk {i+1} failed. Aborting.\"); return None\n",
        "        if not success: return None\n",
        "        if success and i < num_chunks - 1: time.sleep(ERIS_CHUNK_DELAY_SECONDS)\n",
        "    if len(all_fetched_bytes) < total_bytes_needed:\n",
        "        # This warning was a bit confusing, let's clarify.\n",
        "        # If workaround requested more, but API gave less than workaround but MORE than original, it's fine.\n",
        "        # The real error is if final_bytes_to_return (after slicing) is less than total_bytes_needed\n",
        "        pass # Slicing at the end handles this.\n",
        "\n",
        "    final_bytes_to_return = bytes(all_fetched_bytes[:total_bytes_needed])\n",
        "    if len(final_bytes_to_return) < total_bytes_needed:\n",
        "        print(f\"Error: Critical shortage. Fetched enough raw ({len(all_fetched_bytes)}) but after slicing for original {total_bytes_needed}, got {len(final_bytes_to_return)}.\")\n",
        "        return None\n",
        "    # print(f\"Successfully fetched and processed {len(final_bytes_to_return)} bytes for ERIS (originally needed {total_bytes_needed}).\")\n",
        "    return final_bytes_to_return\n",
        "\n",
        "def fetch_seed_from_eris(api_key: str, num_bytes: int = 4) -> int | None:\n",
        "    # print(f\"Fetching {num_bytes} bytes from ERIS for seed generation...\") # Reduced verbosity\n",
        "    seed_bytes = fetch_quantum_bytes(num_bytes, api_key)\n",
        "    if seed_bytes:\n",
        "        seed_int = int.from_bytes(seed_bytes, 'big')\n",
        "        print(f\"ERIS seed bytes: {seed_bytes.hex()}, Integer seed: {seed_int}\")\n",
        "        return seed_int\n",
        "    print(\"Failed to fetch seed from ERIS.\")\n",
        "    return None\n",
        "\n",
        "def quantum_choice_sampler(options: list, num_samples: int, api_key: str) -> list | None:\n",
        "    if not options: print(\"Error: Options list empty for quantum_choice_sampler.\"); return None\n",
        "    if num_samples <= 0: print(\"Error: Num samples must be positive for quantum_choice_sampler.\"); return None\n",
        "    all_quantum_bytes = fetch_quantum_bytes(num_samples, api_key)\n",
        "    if all_quantum_bytes is None: print(\"Failed to fetch quantum bytes for sampling.\"); return None\n",
        "    if len(all_quantum_bytes) != num_samples: print(f\"Error: Byte count mismatch for quantum_choice_sampler. Expected {num_samples}, got {len(all_quantum_bytes)}.\"); return None\n",
        "    num_options = len(options)\n",
        "    selected_indices = [byte % num_options for byte in all_quantum_bytes]\n",
        "    return [options[i] for i in selected_indices]\n",
        "\n",
        "# --- LLM Configuration & Experiment ---\n",
        "MODEL_NAME = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"   # \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\" \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "PRNG_FIXED_SEED = 42\n",
        "NUM_BYTES_FOR_ERIS_SEED = 4\n",
        "MAX_NEW_TOKENS = 150 # Reduced for Mistral 7B to manage memory/time\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K = 50\n",
        "DO_SAMPLE = True\n",
        "\n",
        "PROMPTS = [\n",
        "    #\"Create a screenplay of Ares at the DMV.\",\n",
        "    #\"Write a story of Apollo banished to Modern Earth.\",\n",
        "     \"The ancient prophecy spoke of a dragon that slept under the mountain, but it failed to mention\",\n",
        "     \"Explain the concept of recursion to a child using a story about a friendly robot.\",\n",
        "     \"Write a short poem about the silence of a winter forest.\",\n",
        "    # \"What if the Roman Empire had access to basic steam power?\", # Keeping prompts fewer for faster iteration\n",
        "    # \"Generate a list of three unusual ingredients for a pizza.\"\n",
        "]\n",
        "\n",
        "model_loaded_globally = False # Flag to track if model is loaded\n",
        "\n",
        "def load_model_and_tokenizer_once():\n",
        "    global model, tokenizer, device, model_loaded_globally # Allow modification of global variables\n",
        "\n",
        "    if model_loaded_globally:\n",
        "        # print(f\"Model {MODEL_NAME} and tokenizer already loaded.\") # Reduced verbosity\n",
        "        return True\n",
        "\n",
        "    print(\"*\"*20 + \" GPU Check \" + \"*\"*20)\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CRITICAL ERROR: CUDA (GPU) is not available for PyTorch.\")\n",
        "        print(\"BitsAndBytes 8-bit quantization requires a GPU runtime.\")\n",
        "        print(\"Please go to 'Runtime' -> 'Change runtime type' and select a GPU hardware accelerator.\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"CUDA is available! PyTorch CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    print(f\"Attempting to load LLM: {MODEL_NAME} with 8-bit quantization...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            token=HUGGINGFACE_API_KEY,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # --- Updated Quantization Config ---\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            token=HUGGINGFACE_API_KEY,\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=quantization_config, # Use BitsAndBytesConfig object\n",
        "            device_map=\"auto\"       # Let accelerate handle device mapping\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            # print(f\"Tokenizer pad_token_id is None, setting to eos_token_id: {tokenizer.eos_token_id}\")\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        # For Mistral, this is usually handled well by the model config.\n",
        "        # if model.config.pad_token_id is None:\n",
        "        # model.config.pad_token_id = tokenizer.eos_token_id # Use tokenizer's for consistency\n",
        "\n",
        "        print(f\"LLM {MODEL_NAME} loaded successfully with 8-bit quantization.\")\n",
        "        device = next(model.parameters()).device # Get device from model after device_map\n",
        "        print(f\"Model is on device: {device}\")\n",
        "        model_loaded_globally = True\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM or Tokenizer ({MODEL_NAME}): {e}\")\n",
        "        print(\"Ensure HUGGINGFACE_API_KEY is correctly set and has access if it's a gated model.\")\n",
        "        print(\"Make sure 'bitsandbytes' and 'accelerate' are installed and your runtime has enough RAM/GPU.\")\n",
        "        print(\"If you just installed libraries, YOU MUST RESTART THE RUNTIME (Runtime -> Restart runtime).\")\n",
        "        return False\n",
        "\n",
        "def run_llm_experiment():\n",
        "    global device, tokenizer, model # Ensure we are using the globally loaded tokenizer and model\n",
        "\n",
        "    if OCCYBYTE_API_KEY == \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        print(\"Cannot run experiment: Occybyte API Key is not set.\")\n",
        "        return\n",
        "\n",
        "    if not load_model_and_tokenizer_once(): # This function now sets global tokenizer and model\n",
        "        print(\"Model loading failed. Aborting experiment.\")\n",
        "        return\n",
        "\n",
        "    for i, prompt_text in enumerate(PROMPTS):\n",
        "        print(f\"\\n--- Experiment for Prompt {i+1}/{len(PROMPTS)} ---\")\n",
        "        print(f\"Prompt: \\\"{prompt_text}\\\"\")\n",
        "\n",
        "        if hasattr(tokenizer, 'model_max_length') and isinstance(tokenizer.model_max_length, int) and tokenizer.model_max_length < 1e10:\n",
        "            tokenizer_max_len = tokenizer.model_max_length\n",
        "        elif hasattr(model.config, 'max_position_embeddings') and isinstance(model.config.max_position_embeddings, int):\n",
        "            tokenizer_max_len = model.config.max_position_embeddings\n",
        "        else:\n",
        "            tokenizer_max_len = 512\n",
        "        print(f\"Using tokenizer_max_len: {tokenizer_max_len} for prompt tokenization.\")\n",
        "\n",
        "        tokenized_inputs = tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=tokenizer_max_len\n",
        "        )\n",
        "        input_ids = tokenized_inputs.input_ids.to(device)\n",
        "        attention_mask = tokenized_inputs.attention_mask.to(device)\n",
        "\n",
        "        common_generation_args = {\n",
        "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"top_k\": TOP_K,\n",
        "            \"do_sample\": DO_SAMPLE,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "\n",
        "        # --- ERIS Seeded Generation ---\n",
        "        print(\"\\nGenerating with ERIS Seed...\")\n",
        "        eris_seed_integer = fetch_seed_from_eris(OCCYBYTE_API_KEY, NUM_BYTES_FOR_ERIS_SEED)\n",
        "        if eris_seed_integer is not None:\n",
        "            try:\n",
        "                print(f\"  Setting global torch seed to ERIS-derived: {eris_seed_integer}\")\n",
        "                torch.manual_seed(eris_seed_integer)\n",
        "                if torch.cuda.is_available(): torch.cuda.manual_seed_all(eris_seed_integer)\n",
        "\n",
        "                eris_output_ids = model.generate(input_ids, **common_generation_args)\n",
        "                eris_generated_text = tokenizer.decode(eris_output_ids[0], skip_special_tokens=True)\n",
        "                print(f\"ERIS Output:\\n{eris_generated_text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during ERIS seeded generation: {e}\")\n",
        "        else:\n",
        "            print(\"Could not generate with ERIS seed as seed fetching failed.\")\n",
        "\n",
        "        # --- PRNG Seeded Generation ---\n",
        "        print(\"\\nGenerating with PRNG Fixed Seed...\")\n",
        "        try:\n",
        "            print(f\"  Setting global torch seed to PRNG fixed: {PRNG_FIXED_SEED}\")\n",
        "            torch.manual_seed(PRNG_FIXED_SEED)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(PRNG_FIXED_SEED) # <<< CORRECTED LINE\n",
        "\n",
        "            prng_output_ids = model.generate(input_ids, **common_generation_args)\n",
        "            prng_generated_text = tokenizer.decode(prng_output_ids[0], skip_special_tokens=True)\n",
        "            print(f\"PRNG Output:\\n{prng_generated_text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during PRNG seeded generation: {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This check is crucial for Colab.\n",
        "    if \"google.colab\" in str(get_ipython()): # type: ignore\n",
        "        print(\"Running in Google Colab. Ensure your runtime type is set to GPU for BitsAndBytes quantization.\")\n",
        "        if not torch.cuda.is_available():\n",
        "            print(\"WARNING: GPU NOT DETECTED BY PYTORCH. BitsAndBytes will likely fail.\")\n",
        "            print(\"Go to 'Runtime' -> 'Change runtime type' and select GPU as Hardware Accelerator.\")\n",
        "        else:\n",
        "            print(f\"GPU detected by PyTorch: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    print(\"Starting LLM Experiment with ERIS vs PRNG...\")\n",
        "    run_llm_experiment()\n",
        "\n",
        "    # quantum_choice_sampler example (remains the same)\n",
        "    print(\"\\n--- Example of quantum_choice_sampler (separate from LLM demo) ---\")\n",
        "    if OCCYBYTE_API_KEY != \"YOUR_OCCYBYTE_API_KEY_HERE\":\n",
        "        # ... (rest of your quantum_choice_sampler example call) ...\n",
        "        potential_tokens = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\"]\n",
        "        num_choices_to_make = 5\n",
        "        print(f\"Attempting to select {num_choices_to_make} items from {potential_tokens} using ERIS quantum randomness...\")\n",
        "        selected_items = quantum_choice_sampler(potential_tokens, num_choices_to_make, OCCYBYTE_API_KEY)\n",
        "        if selected_items: print(f\"Quantum-selected items ({len(selected_items)}): {selected_items}\")\n",
        "        else: print(\"Failed to select items using quantum_choice_sampler.\")\n",
        "    else:\n",
        "        print(\"Skipping quantum_choice_sampler example as Occybyte API key is not set.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}